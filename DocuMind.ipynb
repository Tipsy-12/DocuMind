{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQc7k86NcLTJ",
        "outputId": "bb4f9bac-d574-41b8-9e81-256a13ee8cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: camelot-py 1.0.0 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSelecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../01-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../02-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../03-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../04-libgs9-common_9.55.0~dfsg1-0ubuntu5.12_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.12) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../05-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../06-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../07-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../08-libgs9_9.55.0~dfsg1-0ubuntu5.12_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.12) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../09-ghostscript_9.55.0~dfsg1-0ubuntu5.12_amd64.deb ...\n",
            "Unpacking ghostscript (9.55.0~dfsg1-0ubuntu5.12) ...\n",
            "Selecting previously unselected package poppler-utils.\n",
            "Preparing to unpack .../10-poppler-utils_22.02.0-2ubuntu0.9_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.9) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.9) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.12) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.12) ...\n",
            "Setting up ghostscript (9.55.0~dfsg1-0ubuntu5.12) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "#i am showing you my multimodal question answering pdf. evertthing is fine,  i want a summary of the chunk to be shown as context if the context is text, and if the context is table or image then it should be rendered as it is. right the text context consists of the raw text chunks:\n",
        "\n",
        "# =============================\n",
        "# ðŸ“¦ 1. INSTALL DEPENDENCIES\n",
        "# =============================\n",
        "%pip install -Uq \\\n",
        "    langchain langchain-community langchain-core \\\n",
        "    langchain-google-genai==2.1.5 \\\n",
        "    google-ai-generativelanguage==0.6.18 \\\n",
        "    unstructured[all-docs] unstructured[pdf] \\\n",
        "    transformers>=4.30.0 torch>=2.0 \\\n",
        "    chromadb python-dotenv tiktoken \\\n",
        "    docarray camelot-py[cv] langchain-groq\n",
        "\n",
        "!apt-get -qq install poppler-utils ghostscript"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Replace with your actual API keys\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDFGo0BqqOyjuTUlnwmqTC1rThq9WBFhE8\"             # For llama3, mixtral via Groq\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
      ],
      "metadata": {
        "id": "srkMazcxEIu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "file_path = \"/content/Afridi et al. 2020.pdf\"\n",
        "chunks = partition_pdf(\n",
        "    filename=file_path,\n",
        "    infer_table_structure=False,\n",
        "    strategy=\"hi_res\",\n",
        "    extract_image_block_types=[\"Image\"],\n",
        "    extract_image_block_to_payload=True,\n",
        "    chunking_strategy=\"by_title\",\n",
        "    max_characters=10000,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    new_after_n_chars=6000,\n",
        ")\n",
        "\n",
        "\n",
        "texts, tables = [], []\n",
        "paper_title = None\n",
        "\n",
        "for chunk in chunks:\n",
        "    # Capture the first title if not already set\n",
        "    if paper_title is None and chunk.category == \"Title\":\n",
        "        paper_title = chunk.text.strip()\n",
        "\n",
        "    # Separate tables and text\n",
        "    if \"Table\" in str(type(chunk)):\n",
        "        tables.append(chunk)\n",
        "    elif \"CompositeElement\" in str(type(chunk)):\n",
        "        texts.append(chunk)\n",
        "\n",
        "# Fallback if title not found\n",
        "if not paper_title:\n",
        "    paper_title = \"Unknown Paper Title\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l1bfa4KEBaI",
        "outputId": "f991d9e3-b1cc-4048-a3cf-774c30400f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No languages specified, defaulting to English.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Extract base64 images from CompositeElement chunks\n",
        "def get_images_base64(chunks):\n",
        "    images_b64 = []\n",
        "    for chunk in chunks:\n",
        "        if \"CompositeElement\" in str(type(chunk)):\n",
        "            for el in chunk.metadata.orig_elements:\n",
        "                if \"Image\" in str(type(el)):\n",
        "                    images_b64.append(el.metadata.image_base64)\n",
        "    return images_b64\n",
        "\n",
        "images = get_images_base64(chunks)\n",
        "\n",
        "import base64\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def display_base64_image(base64_code):\n",
        "    display(Image(data=base64.b64decode(base64_code)))"
      ],
      "metadata": {
        "id": "OHKgAPhIFLfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract elements\n",
        "elements = partition_pdf(\n",
        "    filename=file_path,\n",
        "    strategy=\"hi_res\",\n",
        "    infer_table_structure=True,\n",
        "    model_name=\"yolox\"\n",
        ")\n",
        "\n",
        "# Keep original Table elements directly\n",
        "tables_only = []\n",
        "\n",
        "for el in elements:\n",
        "    if el.category == \"Table\":  # category is more reliable than type string\n",
        "        tables_only.append(el)  # store the element itself\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGerCyKUErqy",
        "outputId": "2d742bdd-21fd-4400-8863-f2de7738032e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No languages specified, defaulting to English.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tables_only), len(texts), len(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb1av-w3GOUW",
        "outputId": "5462e574-b646-43f9-e95d-0579e02fe643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 19, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarization\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import time\n",
        "\n",
        "# Setup Gemini model (Flash version)\n",
        "model = ChatGroq(model =\"llama-3.1-8b-instant\", temperature=0.3)\n",
        "\n",
        "# Create prompt template for text and table summaries\n",
        "summary_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an assistant tasked with summarizing tables and text.\n",
        "Give a clear and thorough summary of the table or text.\n",
        "\n",
        "Respond only with the summary, no additional comment.\n",
        "Do not start your message with \"Here is a summary\".\n",
        "\n",
        "Table or text chunk: {element}\n",
        "\"\"\")\n",
        "\n",
        "# Create summarization chain\n",
        "summarize_chain = {\"element\": lambda x: x} | summary_prompt | model | StrOutputParser()\n",
        "\n",
        "# ------------------------\n",
        "# 4. Summarize in batches with rate-limit control\n",
        "# ------------------------\n",
        "def summarize_in_batches(texts, batch_size=2, delay=10):\n",
        "    summaries = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        result = summarize_chain.batch(batch, {\"max_concurrency\": batch_size})\n",
        "        summaries.extend(result)\n",
        "        if i + batch_size < len(texts):\n",
        "            print(f\"Sleeping {delay}s to avoid rate limits...\")\n",
        "            time.sleep(delay)\n",
        "    return summaries\n",
        "\n",
        "\n",
        "# Summarize extracted texts\n",
        "text_summaries = summarize_in_batches(texts, batch_size=2, delay=10)\n",
        "table_htmls = [\n",
        "    getattr(t.metadata, \"text_as_html\", \"\") for t in tables_only\n",
        "]\n",
        "table_summaries = summarize_in_batches(tables_only, batch_size=2, delay=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFzfBjXpE4iu",
        "outputId": "a95345fa-a761-4136-bc1d-b9432566c8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sleeping 10s to avoid rate limits...\n",
            "Sleeping 10s to avoid rate limits...\n",
            "Sleeping 10s to avoid rate limits...\n",
            "Sleeping 10s to avoid rate limits...\n",
            "Sleeping 10s to avoid rate limits...\n",
            "Sleeping 10s to avoid rate limits...\n",
            "Sleeping 10s to avoid rate limits...\n",
            "Sleeping 10s to avoid rate limits...\n",
            "Sleeping 10s to avoid rate limits...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries"
      ],
      "metadata": {
        "id": "5lmu_H__GVpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import time\n",
        "\n",
        "# Gemini Flash model\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# Prompt template for image captioning\n",
        "prompt_template = \"\"\"You are a research assistant. Describe the image clearly.\n",
        "This image comes from a paper titled: \"{paper_title}\".\n",
        "Be specific about what is shown if the image if of plots, diagrams, or architecture illustrations.\n",
        "Do not summarize if the image is a human face, simply say that its a human.\n",
        "If its something simple like an animal or a plant or an object, just state what it is.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Wrap as multi-modal prompt template\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def build_prompt_for_image(base64_img, paper_title):\n",
        "    filled_prompt = prompt_template.format(paper_title=paper_title)\n",
        "    return [\n",
        "        HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": filled_prompt},\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"}},\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "# Summarize all images\n",
        "\n",
        "image_summaries = []\n",
        "for image in images:\n",
        "    prompt = build_prompt_for_image(image, paper_title)\n",
        "    chain = model | StrOutputParser()\n",
        "    summary = chain.invoke(prompt)\n",
        "    image_summaries.append(summary)\n"
      ],
      "metadata": {
        "id": "lfb_0fydFaGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_summaries"
      ],
      "metadata": {
        "id": "h9_rpRzXJb8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data and summaries to vectorstore\n",
        "\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "import uuid\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Text chunks\n",
        "id_key = \"doc_id\"\n",
        "text_ids = [str(uuid.uuid4()) for _ in texts]\n",
        "\n",
        "\n",
        "\n",
        "# Text summaries\n",
        "summary_text_docs = [\n",
        "    Document(page_content=summary, metadata={id_key: text_ids[i], \"type\": \"text\"})\n",
        "    for i, summary in enumerate(text_summaries)\n",
        "]\n",
        "\n",
        "vectorstore = DocArrayInMemorySearch.from_documents(\n",
        "    summary_text_docs,\n",
        "    embedding=embedding_function\n",
        ")\n",
        "\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "\n",
        "store = InMemoryStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "\n",
        "store.mset(list(zip(text_ids, texts)))\n",
        "\n",
        "id_key = \"doc_id\"\n",
        "table_ids = [str(uuid.uuid4()) for _ in tables_only]\n",
        "\n",
        "# Table summaries\n",
        "summary_table_docs = [\n",
        "    Document(page_content=summary, metadata={id_key: table_ids[i], \"type\": \"table\"})\n",
        "    for i, summary in enumerate(table_summaries)\n",
        "]\n",
        "\n",
        "\n",
        "vectorstore.add_documents(summary_table_docs)\n",
        "store.mset(list(zip(table_ids, tables_only)))\n",
        "\n",
        "id_key = \"doc_id\"\n",
        "image_ids = [str(uuid.uuid4()) for _ in images]\n",
        "\n",
        "# Image summaries\n",
        "summary_image_docs = [\n",
        "    Document(page_content=summary, metadata={id_key: image_ids[i], \"type\": \"image\"})\n",
        "    for i, summary in enumerate(image_summaries)\n",
        "]\n",
        "\n",
        "vectorstore.add_documents(summary_image_docs)\n",
        "store.mset(list(zip(image_ids, images)))\n"
      ],
      "metadata": {
        "id": "F0-WtjPwFlwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Imports ===\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from base64 import b64decode\n",
        "\n",
        "# === Helper: Separate images and text ===\n",
        "def parse_docs(docs):\n",
        "    b64 = []\n",
        "    text = []\n",
        "    for doc in docs:\n",
        "        try:\n",
        "            b64decode(doc)  # try decode, if valid base64 assume image\n",
        "            b64.append(doc)\n",
        "        except Exception:\n",
        "            text.append(doc)\n",
        "    return {\"images\": b64, \"texts\": text}\n",
        "\n",
        "# === Helper: Build Gemini-compatible multimodal prompt ===\n",
        "def build_prompt(kwargs):\n",
        "    docs_by_type = kwargs[\"context\"]\n",
        "    user_question = kwargs[\"question\"]\n",
        "\n",
        "    context_text = \"\"\n",
        "    for text_element in docs_by_type[\"texts\"]:\n",
        "        context_text += text_element.text\n",
        "\n",
        "    prompt_template = f\"\"\"\n",
        "    Answer the question in thoroughly and in detail based only on the following context, which can include text, tables, and the below image(s).\n",
        "    Context: {context_text}\n",
        "    Question: {user_question}\n",
        "    Respond only with the answer, no additional comment.\n",
        "    Do not start your message with \"Based on the given text\" or \"The provided text\".\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
        "\n",
        "    for image in docs_by_type[\"images\"]:\n",
        "        prompt_content.append({\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}\n",
        "        })\n",
        "\n",
        "    return [HumanMessage(content=prompt_content)]\n",
        "\n",
        "# === Basic Chain: Gemini Flash Multimodal RAG ===\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": retriever | RunnableLambda(parse_docs),\n",
        "        \"question\": RunnablePassthrough(),\n",
        "    }\n",
        "    | RunnableLambda(build_prompt)\n",
        "    | ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# === Chain with Context: Returns both Answer and Used Docs ===\n",
        "chain_with_sources = (\n",
        "    {\n",
        "        \"context\": retriever | RunnableLambda(parse_docs),\n",
        "        \"question\": RunnablePassthrough(),\n",
        "    }\n",
        "    | RunnablePassthrough().assign(\n",
        "        response=(\n",
        "            RunnableLambda(build_prompt)\n",
        "            | ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Fa8Jz9gvF0f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain_with_sources.invoke(\n",
        "    \"What is multihead attention?\"\n",
        ")\n",
        "\n",
        "print(\"Response:\", response['response'])\n",
        "\n",
        "print(\"\\n\\nContext:\")\n",
        "for text in response['context']['texts']:\n",
        "    print(text.text)\n",
        "    print(\"Page number: \", text.metadata.page_number)\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for image in response['context']['images']:\n",
        "    display_base64_image(image)"
      ],
      "metadata": {
        "id": "LsTBcndrF778"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}